# ec2-remediation-system  

A ServiceNow-based incident response system built to help Netflix DevOps quickly respond to AWS EC2 failures. The system automatically creates incidents, surfaces AI-powered remediation steps, sends alerts through Slack, and allows engineers to trigger fixes directly from ServiceNow to reduce downtime and protect the streaming experience.  

---

# Implementation Steps  

## 1. Scoped Application (ServiceNow Studio)  
The implementation began in **ServiceNow Studio**, where I created a scoped application named **EC2 Monitoring and Remediation**.  

Using a scoped application ensured that all custom components were isolated from global changes and could be managed independently.  

The scope name (`x_snc_ec2_monito_0`) was critical because Netflix’s AWS Integration Server depends on this namespace to correctly identify where to push EC2 data for monitoring. Without the proper scope, EC2 instance data would not populate into ServiceNow.  

<img width="789" height="466" alt="image" src="https://github.com/user-attachments/assets/7f8d6786-5716-438d-8560-e5aa5578f531" />


---

## 2. Custom Tables  
Two custom tables were created inside the application to support monitoring and auditing:  

- **EC2 Instance Table**  
  Stores real-time AWS instance information. Required fields include:  
  - Instance Name (String, 40)  
  - Instance ID (String, 40)  
  - Instance Status (String, 40, populated by AWS Integration Server with ON/OFF)  
  - Created / Updated (auto-generated by ServiceNow)

  - The AWS Integration Server automatically populates this table every minute and, for testing, forces the status to OFF every 10 minutes.  
  - This ensures the flow can be consistently validated against simulated EC2 failures.

<img width="793" height="475" alt="image" src="https://github.com/user-attachments/assets/4ecc3268-d7de-44bd-bb5e-ae4c5c3912d4" />


- **Remediation Log Table**  
  - Records every remediation attempt with full audit details. Required fields include:  
  - EC2 Instance (Reference to EC2 Instance table)  
  - Attempted Status (String, 40)  
  - Success (True/False)  
  - Timestamp (Date/Time)  
  - Request Payload (String, 4000)  
  - Response Payload (String, 4000)  
  - HTTP Status Code (Integer)  
  - Error Message (String, 4000)  
  - These logs provide Netflix DevOps with end-to-end visibility into what actions were taken, how AWS responded, and whether the remediation succeeded.  

<img width="771" height="464" alt="image" src="https://github.com/user-attachments/assets/a58d41de-1c86-45b3-bc21-b3b62055183a" />


---

## 3. AWS Integration Setup  
To connect ServiceNow with the AWS Integration Server, I configured secure integration points:  

- **Connection & Credential Alias**: `AWS Integration Server C C Alias`
<img width="791" height="260" alt="image" src="https://github.com/user-attachments/assets/ba7c4cf1-9714-4668-b76a-516d348524d7" />

- **HTTP Connection**: `AWS Integration Server Connection` with the AWS server host and base path.
<img width="782" height="301" alt="image" src="https://github.com/user-attachments/assets/73714f7b-954a-4613-9b7a-21be9fb445c7" />

- **Basic Auth Credential**: `AWS Integration Server Credentials`, which required my ServiceNow instance username and password.  
<img width="795" height="188" alt="image" src="https://github.com/user-attachments/assets/d55e4f36-878d-4053-b577-d81e0ab2b7aa" />

These records abstract sensitive values into reusable credentials so that **Flow Designer** and **Script Includes** can use them without exposing authentication directly in code.  

---

 
## 4. UI Action and Script Include  

### UI Action: Trigger EC2 Remediation 

**Functionality**  
When an engineer clicks the button on an EC2 Instance record, the client script fires. It:  
1. Passes the current record’s `sys_id` to the server.  
2. Calls the `EC2RemediationHelper` Script Include using GlideAjax.  
3. Waits for the server to respond and then notifies the engineer whether the remediation attempt worked or failed.    

 **UI Action Record (Admin View)** 
<img width="792" height="479" alt="image" src="https://github.com/user-attachments/assets/a1551f07-3510-4564-ab00-a3fb4f6d58ea" />


**Form Button (Engineer View)** 

<img width="769" height="199" alt="image" src="https://github.com/user-attachments/assets/add1bac4-01b4-4ab7-b5cd-2d22449369f1" />

---

### Script Include: EC2RemediationHelper  


**Functionality**  
The Script Include contains the logic to actually perform the remediation. When called by the UI Action, it:  
1. Looks up the EC2 Instance record in ServiceNow using the provided `sys_id`.  
2. Pulls the instance details (name, ID, and current status).  
3. Builds a REST API call using the Connection & Credential Alias, HTTP Connection, and Basic Auth configured.  
4. Sends the remediation request to the AWS Integration Server.  
5. Records the request and response in the Remediation Log table, including status codes, payloads, and whether the call succeeded.  
6. Returns a message back to the client so the engineer knows immediately if the remediation worked.  

<img width="818" height="338" alt="image" src="https://github.com/user-attachments/assets/82fe046e-5785-4e60-9085-35cc763b6533" />

 
---

## 5. Workflow Automation (Flow Designer)  
Automation was designed in **Flow Designer**, where a flow triggers whenever an **EC2 Instance status = OFF**.  

<img width="773" height="311" alt="image" src="https://github.com/user-attachments/assets/7c02cfa3-40ee-40fe-9e94-22c3b5a38bd1" />


The flow executes three critical actions:  

### Incident Creation  
A new **Incident record** is automatically generated with full mapping of fields:  

- **Caller**: System Administrator  
- **Category**: Cloud  
- **Subcategory**: EC2  
- **Short Description**: `EC2 Instance Failure`  
- **Description**: This incident was automatically generated by the EC2 Monitoring and Remediation flow. Failure Details:
  Instance Name: ${Instance Name}, Instance ID: ${Instance ID},Current Status: ${Instance Status}
- **Impact**: 1 - High  
- **Urgency**: 1 - High

<img width="518" height="380" alt="image" src="https://github.com/user-attachments/assets/4093b114-3b48-4aa4-a6bd-dab26ce45d09" />

An **Assignment Rule** ensures that incidents with Category = Cloud and Subcategory = EC2 are automatically routed to the **Netflix DevOps assignment group**.  

<img width="788" height="229" alt="image" src="https://github.com/user-attachments/assets/ec362360-fa9e-4f98-a1a5-bf373e37f66f" />

<img width="709" height="219" alt="image" src="https://github.com/user-attachments/assets/997e6df7-6488-4ad1-bccf-6b5a404e5997" />


### AI Search Integration  
The flow uses **AI Search** to automatically query for remediation articles based on EC2-related keywords.  
- Results are attached to the incident.  
- Guidance is also included in notifications for faster troubleshooting.  
- System logs confirm execution of the AI Search step for validation.

<img width="772" height="233" alt="image" src="https://github.com/user-attachments/assets/58ac7afb-733d-48f7-84ca-f938c71c7903" />


### Slack Notification  
The final step in the flow posts a structured message to the Netflix DevOps Slack channel using an incoming webhook.  

The Slack payload includes:  
- Instance Name  
- Instance ID  
- Instance Status
- Incident Number (with confirmation that it has been assigned to the Netflix DevOps team)  
- Remediation Guidance (retrieved via AI Search)  

This ensures engineers receive real-time alerts with both system failure details and actionable remediation instructions without leaving Slack.  

<img width="371" height="377" alt="image" src="https://github.com/user-attachments/assets/2fc197a4-da2c-4763-aa8d-8dff12362724" />


## 6. Knowledge Base and AI Search  
To support AI-driven troubleshooting, I created a Knowledge Base article with a natural language title:  

**“How to remediate a failed AWS EC2 instance from ServiceNow”**  

Details of the article:  
- **Content**: Step-by-step remediation instructions (navigate to the module, open the EC2 Instance, click Trigger EC2 Remediation, review logs).  
- **Keywords**: EC2, server, instance, restart, AWS, virtual machine, cloud server, reboot.  
- **Access Control**: Configured the form layout and added `Can Read` and `Cannot Read` fields so only the Netflix DevOps team could access it.  

<img width="482" height="296" alt="image" src="https://github.com/user-attachments/assets/0cf9367e-cf80-4c56-92ef-e82737bfa273" />
 
---

## 7. Application Access  
For security, I leveraged the default admin role generated with the scoped application:  

1. Added the role to the **EC2 Monitoring and Remediation** application.  
2. Assigned the role to the **Netflix DevOps group**.  

This ensured that only Netflix DevOps engineers had access to the application and its related modules (EC2 Instances and Remediation Logs). No other teams could view or interact with the tables.  

## 8. Testing and Validation  

End-to-end testing confirmed that the system/flow works as expected!  

<img width="182" height="349" alt="image" src="https://github.com/user-attachments/assets/47854a88-e154-4476-86c9-efcf07d752e7" />


# Architecture Diagram
![System Architecture](Diagram.png)  

# Optimization

When I designed the EC2 Monitoring and Remediation system, my focus was not just on functionality but on making it efficient and reliable for the Netflix DevOps team. Here are the key improvements I introduced:

- **Scoped Application for Isolation**  
  Instead of building on global tables, I created a dedicated scoped application. This prevented conflicts with global configurations and ensured all custom tables, flows, and scripts were neatly contained.

- **Automated Workflows Instead of Manual Steps**  
  Previously, engineers would have to manually detect failures and look up remediation guidance. I replaced that process with a Flow Designer flow that automatically creates incidents, attaches remediation guidance via AI Search, and notifies DevOps through Slack in real time. This eliminated delays and reduced manual effort.

- **One-Click Remediation With Full Audit Trail**  
  To reduce downtime, I gave engineers a one-click remediation button directly on the EC2 Instance record. Every action taken through this button is logged in the Remediation Log table, providing accountability and visibility into what happened, when, and why.

- **Credential Security and Reusability**  
  Rather than hard-coding credentials, I used ServiceNow’s Connection & Credential framework. This allows integrations to securely authenticate without exposing usernames or passwords, making the setup both more secure and reusable for future integrations.

- **Role-Based Access for Reliability**  
  I added the application’s default admin role to the Netflix DevOps group. By restricting visibility to only DevOps engineers, the system is more reliable because no outside teams can accidentally trigger or interfere with remediation actions.

Together, these choices improved both efficiency (by cutting down on manual effort and giving engineers instant tools) and reliability (by securing access, logging every action, and preventing conflicts with global changes).

# DevOps Usage

For Netflix DevOps engineers, the workflow looks like this:  

1. **Review Slack Notification**  
   - When an EC2 instance goes **OFF**, a Slack message is posted to the Netflix DevOps channel.  
   - The message includes:  
     - The failing Instance ID  
     - The related Incident number  
     - A summary of AI Search remediation guidance  

2. **Trigger Remediation in ServiceNow**  
   - Navigate to the EC2 Instance record linked in the system.  
   - Click **Trigger EC2 Remediation** to initiate the fix via the AWS Integration Server.  
   - You’ll see an on-screen confirmation showing whether the remediation attempt succeeded or failed.  

3. **Verify in the Remediation Log**  
   - Check the **Remediation Log table** for the full audit trail:  
     - Request and Response payloads  
     - HTTP Status Code (e.g., 201 = success, 401 = unauthorized)  
     - Timestamp and Success flag  
   - This ensures accountability and helps engineers confirm the exact outcome.  

4. **Review and Resolve the Incident**  
   - Open the automatically generated incident record using the number provided in Slack.  
   - Confirm that the Instance has been restored to “ON” status.  
   - Add work notes referencing the remediation attempt and outcome.  
   - If successful, set the Incident to **Resolved**.  
   - If remediation fails, document the failure and escalate to the next support tier.  

